
For train.py: 
import sys
sys.path.insert(0, '~/venvs/env_pt12/lib/python3.8/site-packages')

data_folder_path -> '/N/project/SAIGE_shared/librispeech'


srun -p gpu -A r00105 --gpus-per-node 1 --pty bash
module load python/3.8.10

cd ~/Projects/DiffCodec

python -m srcs.train --enc_ratios 8 5 4 2 --seq_len_p_sec 5 --n_residual_layers 1 --n_filters 32 --lstm 2  --quantization --bandwidth 24 --use_disc --disc_freq 5 --final_activation Tanh --exp_name 0515_encodec_tanh_libri_test --batch_size 20 --lr 0.00005 --data_folder_path '/N/project/SAIGE_shared/librispeech'


python -m srcs.train --lr 0.00005 --seq_len_p_sec 2.4 --rep_dims 128 --diff_dims 256 --enc_ratios 8  --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --exp_name 0626_stack_ups_scl_cond --run_diff --model_type unet --seq_length 4800 --data_folder_path '/N/project/SAIGE_shared/librispeech' --freeze_ed --scaling_frame --self_condition



# 0518_encodec_libri_noqtz
# 0614_encodec_libri_1kb

#!/bin/bash

#SBATCH -J 0516_8_ae_self_cond
#SBATCH -p general
#SBATCH -o filename_%j.txt
#SBATCH -e filename_%j.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=hy17@iu.edu
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --time=20:00:00
#SBATCH --mem=16G
#SBATCH -A r00105

#Load any modules that your program needs
module load python/3.8.10

cd /N/u/hy17/BigRed200/Projects/DiffCodec

#Run your program



python -m srcs.train --enc_ratios 8 5 4 2 --seq_len_p_sec 5 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model /N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0518_encodec_libri_noqtz --quantization --bandwidth 1.5 --use_disc --disc_freq 5 --exp_name 0614_encodec_libri_1_5kb --batch_size 20 --lr 0.00005 --data_folder_path '/N/project/SAIGE_shared/librispeech'

python -m srcs.train --enc_ratios 8 5 4 2 --seq_len_p_sec 5 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model /N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0518_encodec_libri_noqtz --quantization --bandwidth 1 --use_disc --disc_freq 5 --exp_name 0614_encodec_libri_1kb --batch_size 20 --lr 0.00005


python -m srcs.train --lr 0.00005 --seq_len_p_sec 0.8 --rep_dims 128 --diff_dims 256 --enc_ratios 8 --cond_enc_ratios 8 5 4 2 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --model_for_cond '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0518_encodec_libri_noqtz' --scaling_frame --exp_name 0525_cond_ups --batch_size 20 --run_diff --model_type unet --seq_length 1600 --data_folder_path '/N/project/SAIGE_shared/librispeech' --debug

python -m srcs.train --seq_len_p_sec 0.2 --enc_ratios 8 5 4 2 --n_residual_layers 1 --n_filters 32 --lstm 2 --exp_name test --batch_size 20 --data_folder_path '/N/project/SAIGE_shared/librispeech' --lr 0.00005 --quantization --debug

python -m srcs.train --lr 0.00005 --seq_len_p_sec 2.4 --rep_dims 128 --diff_dims 256 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --model_for_cond '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0614_encodec_libri_1kb' --exp_name 0729_time --run_diff --model_type unet --seq_length 38400 --data_folder_path '/N/project/SAIGE_shared/librispeech' --scaling_feature --cond_quantization --cond_bandwidth 1 --debug



# python -m srcs.train --lr 0.00005 --seq_len_p_sec 2.4 --rep_dims 128 --diff_dims 256 --enc_ratios 8  --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --exp_name 0626_stack_ups_self_ft --run_diff --model_type unet --seq_length 4800 --data_folder_path '/N/project/SAIGE_shared/librispeech' --freeze_ed --scaling_feature --self_condition

 
python -m srcs.train --lr 0.00005 --seq_len_p_sec 2.4 --rep_dims 128 --diff_dims 256 --enc_ratios 8 --cond_enc_ratios 8 5 4 2 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --model_for_cond '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0614_encodec_libri_1kb' --exp_name 0823_no_cond_scale_1kb --run_diff --model_type unet --seq_length 4800 --data_folder_path '/N/project/SAIGE_shared/librispeech' --freeze_ed --scaling_global' --cond_quantization --cond_bandwidth 1 

--cond_global 85


python -m srcs.train_dac --lr 0.00005 --seq_len_p_sec 2.4 --rep_dims 128 --diff_dims 256 --enc_ratios 8 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --model_for_cond '44khz' --exp_name 0728_dac_test_glb --run_diff --model_type unet --seq_length 4800 --data_folder_path '/N/project/SAIGE_shared/librispeech' --freeze_ed --scaling_global

python -m srcs.train --lr 0.00005 --seq_len_p_sec 2.4 --rep_dims 128 --diff_dims 256 --n_residual_layers 1 --enc_ratios 8 4 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0807_ae_84' --n_filters 32 --lstm 2 --model_for_cond '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0614_encodec_libri_1kb' --exp_name 0823_diff_84_global --run_diff --model_type unet --seq_length 1200 --data_folder_path '/N/project/SAIGE_shared/librispeech' --scaling_global --cond_quantization --cond_bandwidth 1 --debug

python -m srcs.train_dac --lr 0.00005 --seq_len_p_sec 2.4 --rep_dims 128 --diff_dims 256 --enc_ratios 8 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --model_for_cond '44khz' --exp_name 0729_time --run_diff --model_type unet --seq_length 4800 --data_folder_path '/N/project/SAIGE_shared/librispeech' --freeze_ed --scaling_feature

python -m srcs.train --lr 0.00005 --seq_len_p_sec 2.4 --rep_dims 128 --diff_dims 256 --n_residual_layers 1 --n_filters 32 --lstm 2 --model_for_cond '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0614_encodec_libri_1kb' --exp_name 0729_time_glb --run_diff --model_type unet --seq_length 38400 --data_folder_path '/N/project/SAIGE_shared/librispeech' --scaling_global --cond_quantization --cond_bandwidth 1 --cond_global 85
