
For train.py: 
import sys
sys.path.insert(0, '~/venvs/env_pt12/lib/python3.8/site-packages')

data_folder_path -> '/N/project/SAIGE_shared/librispeech'


srun -p gpu -A r00105 --gpus-per-node 1 --pty bash
module load python/3.8.10

cd ~/Projects/DiffCodec

python -m srcs.train --enc_ratios 8 5 4 2 --seq_len_p_sec 5 --n_residual_layers 1 --n_filters 32 --lstm 2  --quantization --bandwidth 24 --use_disc --disc_freq 5 --final_activation Tanh --exp_name 0515_encodec_tanh_libri_test --batch_size 20 --lr 0.00005 --data_folder_path '/N/project/SAIGE_shared/librispeech'



#!/bin/bash

#SBATCH -J 0516_8_ae_self_cond
#SBATCH -p general
#SBATCH -o filename_%j.txt
#SBATCH -e filename_%j.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=hy17@iu.edu
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --time=20:00:00
#SBATCH --mem=16G
#SBATCH -A r00105

#Load any modules that your program needs
module load python/3.8.10

cd /N/u/hy17/BigRed200/Projects/DiffCodec

#Run your program
python -m srcs.train --lr 0.00005 --seq_len_p_sec 0.8 --rep_dims 128 --diff_dims 256 --enc_ratios 8 --cond_enc_ratios 8 5 4 2 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --model_for_cond '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0518_encodec_libri_noqtz' --scaling_frame --exp_name 0525_cond_ups --batch_size 20 --run_diff --model_type unet --seq_length 1600 --data_folder_path '/N/project/SAIGE_shared/librispeech' --debug

python -m srcs.train --seq_len_p_sec 0.2 --enc_ratios 8 5 4 2 --n_residual_layers 1 --n_filters 32 --lstm 2 --exp_name test --batch_size 20 --data_folder_path '/N/project/SAIGE_shared/librispeech' --lr 0.00005 --quantization --debug

python -m srcs.train --lr 0.00005 --seq_len_p_sec 0.8 --rep_dims 128 --diff_dims 256 --enc_ratios 8 --cond_enc_ratios 8 5 4 2 --n_residual_layers 1 --n_filters 32 --lstm 2 --finetune_model '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/8_ae' --model_for_cond '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0518_encodec_libri_noqtz' --scaling_frame --exp_name 0528_cond_film --batch_size 20 --run_diff --model_type unet --seq_length 1600 --data_folder_path '/N/project/SAIGE_shared/librispeech' --debug


python -m srcs.sample --enc_ratios 8 --seq_len_p_sec 2.4 --n_residual_layers 1 --n_filters 32 --lstm 2 --model_path '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0528_cond_film/model_best.amlt' --seq_length 4800 --model_type unet --rep_dims 128 --diff_dims 256 --data_folder_path '/N/project/SAIGE_shared/librispeech' --scaling_frame --run_diff --cond_enc_ratios 8 5 4 2

python -m srcs.sample --enc_ratios 8 --seq_len_p_sec 2.4 --n_residual_layers 1 --n_filters 32 --lstm 2 --model_path '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0529_cond_film/model_best.amlt' --seq_length 4800 --model_type unet --rep_dims 128 --diff_dims 256 --data_folder_path '/N/project/SAIGE_shared/librispeech' --scaling_frame --run_diff --cond_enc_ratios 8 5 4 2 --model_for_cond '/N/u/hy17/BigRed200/Projects/DiffCodec/saved_models/0518_encodec_libri_noqtz'
